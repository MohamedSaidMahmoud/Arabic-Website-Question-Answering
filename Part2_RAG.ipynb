{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Load Text Files"
      ],
      "metadata": {
        "id": "13fO1U1YWpNz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXFgvvIHWKmE",
        "outputId": "3ead6221-e1b5-4853-899d-98c1ac9ba3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 document(s). Sample:\n",
            "\n",
            "**MainContent:** مما لا شكّ فيه فقد أصبح الذكاء الاصطناعي الشغل الشاغل لغالبية حكومات الدول المتقدّمة؛ لإدراكها أن العالم يقف عند فجر حقبة جديدة، ستغيّر حياة البشرية والطريقة التي تعيش وتعمل بها في عدد كبير من المجالات والقطاعات المختلفة، مع الإقرار في نفس الوقت بأن مخاطر هذه الحقبة لا تزال مجهولة أيضًا. فلا مبالغة بالقول إن العالم دخل فعليًا مرحلة جديدة عنوانها السباق في تكنولوجيا الذكاء الاصطناعي، لكن إذا لم يتم وضع قواعد ناظمة لهذا السباق؛ فإنّه قد يخرج عن السيطرة، ويهدد المعمورة كلها؛ لأنه يثير العديد من المخاطر والتهديدات الأمنية والأخلاقية؛ نتيجة الاعتماد المتزايد عليه. ليس صدفة أن أكبر الاستثمارات فيه التي يقوم بها العملاقان المتنافسان، الولايات المتحدة والصين، هي في هذا المجال كما أن التطبيقات الضخمة لهذا الذكاء، تعد تمهيدًا لحرب ناعمة بين الآلة والإنسان، ولا يمكن قراءة استقالة \"جيفري هينتون\" – الذي يطلق عليه \"الأب الروحي للذكاء الاصطناعي\" من شركة غوغل أوائل مايو 2023 – إلا في سياق \"جرس الإنذار\" الذي ينبّه البشرية إلى تلك المخاطر العديدة المتعلّقة بالذكاء الاصطناعي. التقديرات ت\n"
          ]
        }
      ],
      "source": [
        "# Load the scraped text content from /content/output.txt\n",
        "\n",
        "# List to hold all documents (in this case, one long document)\n",
        "texts = []\n",
        "\n",
        "# Read the file and store it as a single document\n",
        "with open(\"/content/output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "    texts.append(content)\n",
        "\n",
        "print(f\"Loaded {len(texts)} document(s). Sample:\\n\\n{texts[0][:1000]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare the Embedding Model"
      ],
      "metadata": {
        "id": "4ROX9fGBljKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6pqa5Uch2Wf",
        "outputId": "73f7b697-0872-4cff-ae13-8ba53b4568fc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Embedding class to handle document and query embeddings\n",
        "class EmbeddingModel:\n",
        "    def __init__(self):\n",
        "        # Load multilingual transformer model\n",
        "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2').to(device)\n",
        "\n",
        "    def embed_documents(self, docs):\n",
        "        # Convert documents into embeddings\n",
        "        embeddings = self.model.encode(docs, convert_to_tensor=True, device=device)\n",
        "        return embeddings.cpu().numpy().tolist()\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        # Convert single query into embedding\n",
        "        embeddings = self.model.encode(query, convert_to_tensor=True, device=device)\n",
        "        return embeddings.cpu().numpy().tolist()\n",
        "\n",
        "# Initialize the embedding model\n",
        "embed_model = EmbeddingModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UwqcW7-jB4l",
        "outputId": "9d50c322-7c03-4bc6-c4fd-463e051ce5bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Chroma Vector Database"
      ],
      "metadata": {
        "id": "W_di95nsloqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_chroma"
      ],
      "metadata": {
        "id": "mtAb59ndjsZE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Function to embed documents in batches and store in Chroma vector DB\n",
        "def batch_upsert(texts, batch_size, embed_model):\n",
        "    vector_database = None\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        embeddings = embed_model.embed_documents(batch_texts)\n",
        "\n",
        "        if vector_database is None:\n",
        "            vector_database = Chroma.from_texts(\n",
        "                texts=batch_texts,\n",
        "                embedding=embed_model,\n",
        "                metadatas=None,\n",
        "                ids=None,\n",
        "            )\n",
        "        else:\n",
        "            vector_database.add_texts(\n",
        "                texts=batch_texts,\n",
        "                embeddings=embeddings\n",
        "            )\n",
        "    return vector_database\n",
        "\n",
        "# Set batch size and run upsert\n",
        "batch_size = 20000\n",
        "vector_database = batch_upsert(texts, batch_size, embed_model)\n",
        "\n",
        "# Convert vector database to retriever\n",
        "retriever = vector_database.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n"
      ],
      "metadata": {
        "id": "C5FLfBwkjqYz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the Retriever Output"
      ],
      "metadata": {
        "id": "kTJMgWu6lxbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try retrieving similar articles using a sample question\n",
        "sample_question = \"ما هي أسباب خطورة الذكاء الاصطناعي؟\"\n",
        "similar_docs = retriever.invoke(sample_question)\n",
        "\n",
        "# Print the top 3 similar documents\n",
        "for i, doc in enumerate(similar_docs):\n",
        "    print(f\"\\nArticle {i+1}:\\n{doc.page_content[:500]}...\")  # Print first 500 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6lx4majzzV",
        "outputId": "3f217cd0-7c8d-48fd-81f8-a0a583a6a208"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Article 1:\n",
            "**MainContent:** مما لا شكّ فيه فقد أصبح الذكاء الاصطناعي الشغل الشاغل لغالبية حكومات الدول المتقدّمة؛ لإدراكها أن العالم يقف عند فجر حقبة جديدة، ستغيّر حياة البشرية والطريقة التي تعيش وتعمل بها في عدد كبير من المجالات والقطاعات المختلفة، مع الإقرار في نفس الوقت بأن مخاطر هذه الحقبة لا تزال مجهولة أيضًا. فلا مبالغة بالقول إن العالم دخل فعليًا مرحلة جديدة عنوانها السباق في تكنولوجيا الذكاء الاصطناعي، لكن إذا لم يتم وضع قواعد ناظمة لهذا السباق؛ فإنّه قد يخرج عن السيطرة، ويهدد المعمورة كلها؛ لأنه ي...\n",
            "\n",
            "Article 2:\n",
            "**MainContent:** مما لا شكّ فيه فقد أصبح الذكاء الاصطناعي الشغل الشاغل لغالبية حكومات الدول المتقدّمة؛ لإدراكها أن العالم يقف عند فجر حقبة جديدة، ستغيّر حياة البشرية والطريقة التي تعيش وتعمل بها في عدد كبير من المجالات والقطاعات المختلفة، مع الإقرار في نفس الوقت بأن مخاطر هذه الحقبة لا تزال مجهولة أيضًا. فلا مبالغة بالقول إن العالم دخل فعليًا مرحلة جديدة عنوانها السباق في تكنولوجيا الذكاء الاصطناعي، لكن إذا لم يتم وضع قواعد ناظمة لهذا السباق؛ فإنّه قد يخرج عن السيطرة، ويهدد المعمورة كلها؛ لأنه ي...\n",
            "\n",
            "Article 3:\n",
            "**MainContent:** مما لا شكّ فيه فقد أصبح الذكاء الاصطناعي الشغل الشاغل لغالبية حكومات الدول المتقدّمة؛ لإدراكها أن العالم يقف عند فجر حقبة جديدة، ستغيّر حياة البشرية والطريقة التي تعيش وتعمل بها في عدد كبير من المجالات والقطاعات المختلفة، مع الإقرار في نفس الوقت بأن مخاطر هذه الحقبة لا تزال مجهولة أيضًا. فلا مبالغة بالقول إن العالم دخل فعليًا مرحلة جديدة عنوانها السباق في تكنولوجيا الذكاء الاصطناعي، لكن إذا لم يتم وضع قواعد ناظمة لهذا السباق؛ فإنّه قد يخرج عن السيطرة، ويهدد المعمورة كلها؛ لأنه ي...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build the Prompt Template and RAG Chain"
      ],
      "metadata": {
        "id": "q6H1JSKwl1Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    google_api_key=\"AIzaSyBMrcWQfNlHxyQNzhj0cOWyebJkSBHhS-g\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for the question answering\n",
        "template = \"\"\"\n",
        "You are an intelligent and contextually aware Arabic AI assistant designed to provide precise and detailed responses to customer questions. Your task is to use the information retrieved from the knowledge base to generate a comprehensive and accurate answer.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Contextual Information:\n",
        "{context}\n",
        "\n",
        "Provide a clear and concise Arabic answer based on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format the context from documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Build the final RAG chain\n",
        "context_chain = retriever | format_docs\n",
        "rag_chain = (\n",
        "    {\"context\": context_chain, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "B89FwwoMj7JV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Try the Full RAG Chain on a Sample Question"
      ],
      "metadata": {
        "id": "owSNrgcHl6rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question\n",
        "question = \"ما هي التحديات الرئيسية التي تواجه الذكاء الاصطناعي؟\"\n",
        "\n",
        "# Use the RAG chain to get an answer\n",
        "answer = rag_chain.invoke(question)\n",
        "\n",
        "# Print the answer\n",
        "print(\"Answer to the Question:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fPdspp4kACE",
        "outputId": "4cfda10d-9896-4d49-d6c5-53b97c1d8f7e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer to the Question:\n",
            "\n",
            "تواجه تقنية الذكاء الاصطناعي العديد من التحديات الرئيسية، أبرزها:\n",
            "\n",
            "* **مخاطر فقدان الوظائف:**  يتوقع فقدان مئات الملايين من الوظائف خلال السنوات القادمة بسبب استبدال البشر بالذكاء الاصطناعي في العديد من القطاعات،  بل وحتى  استبداله في 99% من الوظائف خلال عقدين بحسب بعض التوقعات.\n",
            "\n",
            "* **المخاطر الأمنية والأخلاقية:**  يُثير الاعتماد المتزايد على الذكاء الاصطناعي مخاوف أمنية وأخلاقية كبيرة، خاصةً في المجالات العسكرية والدفاعية،  حيث يُمكن استخدامه كأداة سيطرة وفرض نفوذ.  كما أن  غياب القواعد الناظمة  يُمكن أن يُخرج هذا السباق التكنولوجي عن السيطرة.\n",
            "\n",
            "* **التنافس الدولي:**  يشهد العالم سباقًا محمومًا بين الدول والشركات العملاقة لتطوير تقنيات الذكاء الاصطناعي، مما يُثير مخاوف من حرب ناعمة بين الآلة والإنسان،  وتُمثل  الاستثمارات الضخمة من قِبل الولايات المتحدة والصين خير دليل على ذلك.\n",
            "\n",
            "* **التحديات الأخلاقية والمسؤولية:**  يُطرح سؤال المسؤولية الأخلاقية  بشكلٍ كبير  في ظل التطورات السريعة لتقنيات الذكاء الاصطناعي،  و تأثيره الواسع في المجتمع.\n",
            "\n",
            "* **التحديات العربية:**  تُعاني العديد من الدول العربية من  تباطؤ في تبني تقنيات الذكاء الاصطناعي،  مما يُهدد  بفقدانها  فرص التقدم الاقتصادي في ظل التحول نحو الاقتصاد المعرفي الرقمي.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradio UI"
      ],
      "metadata": {
        "id": "I3ixoDv3l9hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGuVc5RflIkk",
        "outputId": "8975a1c3-fedf-4817-e574-320690eb7699"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, safehttpx, gradio-client, gradio\n",
            "Successfully installed aiofiles-24.1.0 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 tomlkit-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rag_interface(question):\n",
        "    articles, answer = process_question(question)\n",
        "    articles_str = \"\\n\\n\".join(articles)\n",
        "    return articles_str, answer\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=rag_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Most Similar Articles\"),\n",
        "        gr.Textbox(label=\"Answer to the Question\")\n",
        "    ],\n",
        "    title=\"Arabic Question Answering System\",\n",
        "    description=\"Enter a question to receive the most similar articles and an answer.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "CU5K2yGalA-x",
        "outputId": "a21e3259-8409-45d8-e71a-b1a7f244d1b3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f2cefb2f220c490ed7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f2cefb2f220c490ed7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save the Answer to a Text File"
      ],
      "metadata": {
        "id": "e9rMYwbqmiTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the answer to a text file\n",
        "ans = process_question(\"ما هي الأسباب التي تجعل الذكاء الاصطناعي يُعد خطرًا على البشرية؟\")\n",
        "\n",
        "with open(\"/content/answer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(ans[1])  # تأكد من أن ans[1] هي الإجابة النهائية\n",
        "\n",
        "print(\"Answer has been saved to /content/answer.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENg4UoSomg9R",
        "outputId": "d8580f33-2b5b-4fb5-92e6-9832ee3d3979"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer has been saved to /content/answer.txt\n"
          ]
        }
      ]
    }
  ]
}